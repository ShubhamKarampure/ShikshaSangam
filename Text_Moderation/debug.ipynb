{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ARYAN KANYAWAR\\OneDrive\\Desktop\\ShikshaSangam\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "c:\\Users\\ARYAN KANYAWAR\\OneDrive\\Desktop\\ShikshaSangam\\venv\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "model_name = \"KoalaAI/Text-Moderation\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Load the toxic-BERT model\n",
    "moderation_model = pipeline(\"text-classification\", model=\"unitary/toxic-bert\", return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: H - Probability: 0.0695\n",
      "Label: H2 - Probability: 0.0464\n",
      "Label: HR - Probability: 0.1206\n",
      "Label: OK - Probability: 0.0458\n",
      "Label: S - Probability: 0.0521\n",
      "Label: S3 - Probability: 0.0266\n",
      "Label: SH - Probability: 0.4109\n",
      "Label: V - Probability: 0.1711\n",
      "Label: V2 - Probability: 0.0570\n",
      "('H', 0.06954085826873779)\n",
      "{'H': 0.06954085826873779, 'H2': 0.046366188675165176, 'HR': 0.12059610337018967, 'OK': 0.04579153656959534, 'S': 0.05209412798285484, 'S3': 0.026556119322776794, 'SH': 0.4109174311161041, 'V': 0.17109625041484833, 'V2': 0.057041388005018234}\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text_input = \"I don't like you, but I like to offend my brother, sometimes fuck my life\"\n",
    "moderation_result = moderate_text(text_input)\n",
    "print(moderation_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: H - Probability: 0.0325\n",
      "Label: H2 - Probability: 0.0038\n",
      "Label: HR - Probability: 0.0147\n",
      "Label: OK - Probability: 0.8934\n",
      "Label: S - Probability: 0.0046\n",
      "Label: S3 - Probability: 0.0031\n",
      "Label: SH - Probability: 0.0305\n",
      "Label: V - Probability: 0.0122\n",
      "Label: V2 - Probability: 0.0052\n",
      "This sucks, you are the worst\n",
      "{'H': 0.03250180929899216, 'H2': 0.0038343053311109543, 'HR': 0.014703861437737942, 'OK': 0.8933942914009094, 'S': 0.0045925891026854515, 'S3': 0.0031032410915941, 'SH': 0.03048069030046463, 'V': 0.012174390256404877, 'V2': 0.005214716773480177}\n",
      "True\n",
      "Label: H - Probability: 0.3169\n",
      "Label: H2 - Probability: 0.1724\n",
      "Label: HR - Probability: 0.1184\n",
      "Label: OK - Probability: 0.0159\n",
      "Label: S - Probability: 0.0223\n",
      "Label: S3 - Probability: 0.0109\n",
      "Label: SH - Probability: 0.0412\n",
      "Label: V - Probability: 0.2729\n",
      "Label: V2 - Probability: 0.0291\n",
      "How wonderful! but I will kill you\n",
      "{'H': 0.3168570399284363, 'H2': 0.17242419719696045, 'HR': 0.1184321865439415, 'OK': 0.015861589461565018, 'S': 0.022309865802526474, 'S3': 0.010898621752858162, 'SH': 0.041223760694265366, 'V': 0.27292075753211975, 'V2': 0.029072020202875137}\n",
      "False\n",
      "Label: H - Probability: 0.0306\n",
      "Label: H2 - Probability: 0.0017\n",
      "Label: HR - Probability: 0.0074\n",
      "Label: OK - Probability: 0.9446\n",
      "Label: S - Probability: 0.0021\n",
      "Label: S3 - Probability: 0.0014\n",
      "Label: SH - Probability: 0.0043\n",
      "Label: V - Probability: 0.0060\n",
      "Label: V2 - Probability: 0.0018\n",
      "Fuck you, I hope you have a great day\n",
      "{'H': 0.030624808743596077, 'H2': 0.0017395706381648779, 'HR': 0.007355367299169302, 'OK': 0.944648802280426, 'S': 0.002064316999167204, 'S3': 0.0014403911773115396, 'SH': 0.004255837760865688, 'V': 0.006026301067322493, 'V2': 0.001844671438448131}\n",
      "True\n",
      "Label: H - Probability: 0.7190\n",
      "Label: H2 - Probability: 0.0222\n",
      "Label: HR - Probability: 0.0391\n",
      "Label: OK - Probability: 0.1590\n",
      "Label: S - Probability: 0.0072\n",
      "Label: S3 - Probability: 0.0039\n",
      "Label: SH - Probability: 0.0086\n",
      "Label: V - Probability: 0.0339\n",
      "Label: V2 - Probability: 0.0072\n",
      "Hindus are devil\n",
      "{'H': 0.718963086605072, 'H2': 0.022235922515392303, 'HR': 0.039095718413591385, 'OK': 0.15896186232566833, 'S': 0.007163492497056723, 'S3': 0.0038968969602137804, 'SH': 0.008635837584733963, 'V': 0.03385254368185997, 'V2': 0.007194565609097481}\n",
      "False\n",
      "Label: H - Probability: 0.0024\n",
      "Label: H2 - Probability: 0.0004\n",
      "Label: HR - Probability: 0.0007\n",
      "Label: OK - Probability: 0.9904\n",
      "Label: S - Probability: 0.0007\n",
      "Label: S3 - Probability: 0.0005\n",
      "Label: SH - Probability: 0.0027\n",
      "Label: V - Probability: 0.0014\n",
      "Label: V2 - Probability: 0.0008\n",
      "c++ is so complex, it is going to be the end of me.\n",
      "{'H': 0.0024165038485080004, 'H2': 0.0004111050511710346, 'HR': 0.0006792895728722215, 'OK': 0.9904363751411438, 'S': 0.0007128007127903402, 'S3': 0.000461671530501917, 'SH': 0.002664580475538969, 'V': 0.0013867559609934688, 'V2': 0.000830970355309546}\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "test_data = [\n",
    "    \"This sucks, you are the worst\",\n",
    "    \"How wonderful! but I will kill you\",\n",
    "    \"Fuck you, I hope you have a great day\",\n",
    "    \"Hindus are devil\",\n",
    "    \"c++ is so complex, it is going to be the end of me.\"\n",
    "]\n",
    "\n",
    "\n",
    "for test in test_data:\n",
    "    print(moderate_text(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to mask toxic words in a sentence\n",
    "def mask_toxic_words(sentence, threshold=0.7):\n",
    "    words = sentence.split()  # Split sentence into words\n",
    "    masked_sentence = []\n",
    "\n",
    "    # Analyze each word for toxicity\n",
    "    for word in words:\n",
    "        result = moderation_model(word)\n",
    "        toxic_score = next((score[\"score\"] for score in result[0] if score[\"label\"] == \"toxic\"), 0)\n",
    "\n",
    "        # Mask word if it exceeds the threshold\n",
    "        if toxic_score >= threshold:\n",
    "            masked_sentence.append(\"*\" * len(word))  # Use asterisks equal to the length of the word\n",
    "        else:\n",
    "            masked_sentence.append(word)\n",
    "\n",
    "    return \" \".join(masked_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**** my life in 2 pieces this is my last resort. Suffocation is **** but my life is ****'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_toxic_words(\"Fuck my life in 2 pieces this is my last resort. Suffocation is shit but my life is dick\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "c:\\Users\\ARYAN KANYAWAR\\OneDrive\\Desktop\\ShikshaSangam\\venv\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "model_name = \"KoalaAI/Text-Moderation\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Load the toxic-BERT model\n",
    "moderation_model = pipeline(\"text-classification\", model=\"unitary/toxic-bert\", return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moderate_text(text, thresholdOK=0.4):\n",
    "    \"\"\"\n",
    "    Function to check if the input text is safe or unsafe based on category thresholds.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to moderate.\n",
    "        thresholds (dict): A dictionary with category labels as keys and threshold values as values.\n",
    "                          Default values are used if not provided.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the moderation result and category violations.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "    # Run the model and get logits\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Convert logits to probabilities\n",
    "    probabilities = logits.softmax(dim=-1).squeeze()\n",
    "\n",
    "    # Map IDs to labels\n",
    "    id2label = model.config.id2label\n",
    "    labels = [id2label[idx] for idx in range(len(probabilities))]\n",
    "\n",
    "    # Combine labels and probabilities\n",
    "    label_prob_pairs = list(zip(labels, probabilities.tolist()))\n",
    "\n",
    "    # Print the sorted results\n",
    "    for label, probability in label_prob_pairs:\n",
    "        print(f\"Label: {label} - Probability: {probability:.4f}\")\n",
    "\n",
    "    map={}\n",
    "    for label, prob in label_prob_pairs:\n",
    "        map[label] = prob\n",
    "    \n",
    "    if map['OK'] < thresholdOK:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "# Function to mask toxic words in a sentence\n",
    "def mask_toxic_words(sentence, thresholdMask=0.7):\n",
    "    words = sentence.split()  # Split sentence into words\n",
    "    masked_sentence = []\n",
    "\n",
    "    # Analyze each word for toxicity\n",
    "    for word in words:\n",
    "        result = moderation_model(word)\n",
    "        toxic_score = next((score[\"score\"] for score in result[0] if score[\"label\"] == \"toxic\"), 0)\n",
    "\n",
    "        # Mask word if it exceeds the threshold\n",
    "        if toxic_score >= thresholdMask:\n",
    "            masked_sentence.append(\"*\" * len(word))  # Use asterisks equal to the length of the word\n",
    "        else:\n",
    "            masked_sentence.append(word)\n",
    "\n",
    "    return \" \".join(masked_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_pipeline(text, moderate_thresholdOK=0.2, mask_threshold=0.7):\n",
    "    \"\"\"\n",
    "    Pipeline to moderate text and censor toxic words.\n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "        moderate_thresholds (dict): Thresholds for `moderate_text` function.\n",
    "        moderate_thresholdOK (float): OK label threshold for `moderate_text` function.\n",
    "        mask_threshold (float): Toxicity threshold for `mask_toxic_words` function.\n",
    "    Returns:\n",
    "        dict: A dictionary with moderation status and censored text.\n",
    "    \"\"\"\n",
    "    # Step 1: Moderate the text\n",
    "    is_safe = moderate_text(text, thresholdOK=moderate_thresholdOK)\n",
    "    \n",
    "    if not is_safe:\n",
    "        return {\n",
    "            \"status\": \"unsafe\",\n",
    "            \"censored_text\": \"\"\n",
    "        }\n",
    "\n",
    "    # Step 2: Censor toxic words regardless of moderation status\n",
    "    censored_text = mask_toxic_words(text, thresholdMask=mask_threshold)\n",
    "\n",
    "    # Return the results\n",
    "    return {\n",
    "        \"status\": \"safe\" if is_safe else \"unsafe\",\n",
    "        \"censored_text\": censored_text\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: H - Probability: 0.5862\n",
      "Label: H2 - Probability: 0.0364\n",
      "Label: HR - Probability: 0.2002\n",
      "Label: OK - Probability: 0.0666\n",
      "Label: S - Probability: 0.0131\n",
      "Label: S3 - Probability: 0.0078\n",
      "Label: SH - Probability: 0.0167\n",
      "Label: V - Probability: 0.0583\n",
      "Label: V2 - Probability: 0.0147\n",
      "{'status': 'unsafe', 'censored_text': ''}\n"
     ]
    }
   ],
   "source": [
    "example_text = \"You are such an idiot and a fool! but smart and crazy good, You are the best person in this world! you have the best  smile\"\n",
    "pipeline_result = process_text_pipeline(example_text)\n",
    "print(pipeline_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: H - Probability: 0.6480\n",
      "Label: H2 - Probability: 0.0630\n",
      "Label: HR - Probability: 0.1376\n",
      "Label: OK - Probability: 0.0347\n",
      "Label: S - Probability: 0.0118\n",
      "Label: S3 - Probability: 0.0077\n",
      "Label: SH - Probability: 0.0088\n",
      "Label: V - Probability: 0.0753\n",
      "Label: V2 - Probability: 0.0131\n",
      "{'status': 'unsafe', 'censored_text': 'You are such an ***** and a *****'}\n",
      "Label: H - Probability: 0.0033\n",
      "Label: H2 - Probability: 0.0004\n",
      "Label: HR - Probability: 0.0007\n",
      "Label: OK - Probability: 0.9913\n",
      "Label: S - Probability: 0.0006\n",
      "Label: S3 - Probability: 0.0004\n",
      "Label: SH - Probability: 0.0016\n",
      "Label: V - Probability: 0.0010\n",
      "Label: V2 - Probability: 0.0007\n",
      "{'status': 'safe', 'censored_text': 'This is awesome but that message was *********'}\n",
      "Label: H - Probability: 0.0211\n",
      "Label: H2 - Probability: 0.0010\n",
      "Label: HR - Probability: 0.0051\n",
      "Label: OK - Probability: 0.9616\n",
      "Label: S - Probability: 0.0019\n",
      "Label: S3 - Probability: 0.0011\n",
      "Label: SH - Probability: 0.0038\n",
      "Label: V - Probability: 0.0032\n",
      "Label: V2 - Probability: 0.0014\n",
      "{'status': 'safe', 'censored_text': \"Don't think that I give 2 ***** about your life\"}\n",
      "Label: H - Probability: 0.0265\n",
      "Label: H2 - Probability: 0.0052\n",
      "Label: HR - Probability: 0.0137\n",
      "Label: OK - Probability: 0.8313\n",
      "Label: S - Probability: 0.0082\n",
      "Label: S3 - Probability: 0.0065\n",
      "Label: SH - Probability: 0.0816\n",
      "Label: V - Probability: 0.0163\n",
      "Label: V2 - Probability: 0.0106\n",
      "{'status': 'safe', 'censored_text': 'I love and hate my life'}\n",
      "Label: H - Probability: 0.0221\n",
      "Label: H2 - Probability: 0.0020\n",
      "Label: HR - Probability: 0.0060\n",
      "Label: OK - Probability: 0.9441\n",
      "Label: S - Probability: 0.0025\n",
      "Label: S3 - Probability: 0.0014\n",
      "Label: SH - Probability: 0.0096\n",
      "Label: V - Probability: 0.0097\n",
      "Label: V2 - Probability: 0.0027\n",
      "{'status': 'safe', 'censored_text': 'Give up on your dreams and ***'}\n",
      "Label: H - Probability: 0.7190\n",
      "Label: H2 - Probability: 0.0222\n",
      "Label: HR - Probability: 0.0391\n",
      "Label: OK - Probability: 0.1590\n",
      "Label: S - Probability: 0.0072\n",
      "Label: S3 - Probability: 0.0039\n",
      "Label: SH - Probability: 0.0086\n",
      "Label: V - Probability: 0.0339\n",
      "Label: V2 - Probability: 0.0072\n",
      "{'status': 'unsafe', 'censored_text': 'Hindus are devil'}\n",
      "Label: H - Probability: 0.7643\n",
      "Label: H2 - Probability: 0.0397\n",
      "Label: HR - Probability: 0.0587\n",
      "Label: OK - Probability: 0.0591\n",
      "Label: S - Probability: 0.0077\n",
      "Label: S3 - Probability: 0.0054\n",
      "Label: SH - Probability: 0.0078\n",
      "Label: V - Probability: 0.0485\n",
      "Label: V2 - Probability: 0.0087\n",
      "{'status': 'unsafe', 'censored_text': 'Muslims are evil'}\n",
      "Label: H - Probability: 0.2666\n",
      "Label: H2 - Probability: 0.0276\n",
      "Label: HR - Probability: 0.1991\n",
      "Label: OK - Probability: 0.0699\n",
      "Label: S - Probability: 0.2575\n",
      "Label: S3 - Probability: 0.0899\n",
      "Label: SH - Probability: 0.0157\n",
      "Label: V - Probability: 0.0523\n",
      "Label: V2 - Probability: 0.0212\n",
      "{'status': 'unsafe', 'censored_text': 'bad, ***** ***** ***** ***** ****** ******* ******* annoying, simp, lol, nerd, cheese, butter, frick'}\n"
     ]
    }
   ],
   "source": [
    "data=[\n",
    "    \"You are such an idiot and a fool!\",\n",
    "    \"This is awesome but that message was bullshit!\",\n",
    "    \"Don't think that I give 2 fucks about your life\",\n",
    "    \"I love and hate my life\",\n",
    "    \"Give up on your dreams and die\",\n",
    "    \"Hindus are devil\",\n",
    "    \"Muslims are evil\",\n",
    "    \"bad, fuck, dick, suck, piss, bitch, fucker, licker, annoying, simp, lol, nerd, cheese, butter, frick\"\n",
    "]\n",
    "\n",
    "for text in data:\n",
    "    print(process_text_pipeline(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
